% NIH grant proposal file (2011)

\documentclass[12pt]{article}

% Packages to load
\usepackage{ulem}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{comment}

% Arial font that NIH allows
\usepackage{fontspec}
\setmainfont{Arial}
%\renewcommand{\familydefault}{\sfdefault}
%\linespread{1.02}

% Better and richer math environment
\usepackage{amsmath}

% EPS and PDF figures
\usepackage{graphicx}

% Make 0.5'' margins on all sides
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}

% Add itemize*, description*, and enumerate* environments to shrink white space between list items
%\usepackage{mdwlist}

\setlength{\baselineskip}{12.6pt} % in text mode
\setlength{\normalbaselineskip}{12.6pt} % in math mode

% No page numbers
\pagestyle{empty}

% Compress white space around titles
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*1}
\titlespacing{\subsection}{0pt}{*0}{*1}
\titlespacing{\subsubsection}{0pt}{*.5}{*.5}
\titlespacing{\paragraph}{0pt}{*0}{*2}

% Separate new paragraphs by 0.2 cm of white space (rather than indents)
\usepackage{parskip}
\setlength{\parskip}{0.2cm}

\setlength{\belowcaptionskip}{-.05ex} % remove extra space above and below in-line float (e.g., captions)
\setlength{\abovecaptionskip}{1.5ex} % remove extra space above and below in-line float (e.g. captions)

% title page info
\title{How Efficient is the Market for Physician Referrals?}
\author{PI: Ian McCarthy, PhD \\
Co-PI: Seth Richards-Shubik, PhD}

\date{August 2022}

% Begin document

\begin{document}
\section{Introduction}
\vspace{.02in}

Hospital and physician services constitute the two largest components of U.S. health expenditures and jointly accounted for nearly \$2 trillion in U.S. health spending in 2019 (52\% of total health expenditures); however, these expenditures are not distributed uniformly across geographic areas \cite{wennberg1973, gottlieb2010, miller2011, wennberg2003}. Rather, expenditures are characterized by areas of very high health care utilization alongside areas of very low utilization, and there is strong empirical evidence that a large share of this geographic variation is not driven by patient preferences \cite{zuckerman2010,finkelstein2016} or by differences in quality of care \cite{skinner1997,baicker2004ha}. Authors estimate that as much as 60\% of residual geographic variation in health care expenditures can be explained by provider behaviors as opposed to patient health care needs \cite{finkelstein2016}, and among this, physician practice style can explain as much as 50\% \cite{molitor2018}. Relatedly, there exists substantial variation in quality and costs across physicians \textit{within the same geographic area} \cite{cooper2019, epstein2009, moy2020}. The conclusion from this literature is that \textbf{a large share of otherwise unexplained variation in health care expenditures and quality of care is driven by provider behaviors}, facilitated by physicians' underlying influence on treatment decisions and location of care; however, policy solutions to remove this variation are elusive and typically require significant changes in physician behaviors. 

Our proposal considers referral patterns from primary care physicians (PCPs) to specialists as an important contributor to these existing inefficiencies and observed within-market variation in spending and quality. Given the PCP's influence on patients' health care decisions, referral networks are a natural candidate for examining sources of and potential solutions to such variation, for two major reasons: 1) there is ample opportunity to improve patient health and lower health care costs if PCPs can better direct patients to more efficient and higher quality specialists; and 2) adjusting PCP referrals is an arguably more realistic and actionable way to reduce variation in care due to differential provider behaviors, rather than attempting to adjust specialist practice styles. We posit that a \textbf{substantial amount of variation in health care quality and spending would be removed if PCPs could more quickly identify the highest-quality and most efficient specialists in their markets and refer patients accordingly.} 

\vspace{.1in}
\section{Objectives}

\vspace{.05in}
\paragraph{Objective 1:} \textit{Describe PCP referral networks empirically and examine the association between salient network statistics and measures of health care quality and cost.}

We will construct referral networks between PCPs and specialists using Medicare claims data, focusing on major joint surgery where PCP referrals are known to heavily influence a patient's choice of orthopedic surgeon. From these networks, we will compute key statistics such as degree centrality and network density, along with measures of referral concentration (e.g., the proportion of a PCP's patients sent to a given specialist). We will examine the association between these network statistics and measures of costs, quality, and utilization. We will also describe the evolution of these referral networks over time, with particular attention to the role of health system integration. Successfully completing \textbf{Objective 1} will offer a comprehensive description of PCP referral networks in the area of orthopedic surgery and major joint replacements.

\vspace{.05in}
\paragraph{Objective 2:} \textit{Estimate the responsiveness of PCP referrals to signals about specialist quality, and examine the implications for patient health and Medicare spending using a model of physician learning.}

We will quantify the relationship between PCP referrals and specialist quality, focusing specifically on changes in PCP referrals following negative patient outcomes. We will also examine this relationship in the context of a structural learning model in which PCPs learn about specialist quality over time and update referral patterns accordingly. Successfully completing \textbf{Objective 2} will quantify the improvement in quality and reduction in spending if PCPs could learn about orthopedic specialist quality more rapidly and thereby improve the allocation of patients to specialists.

\vspace{.1in}
\section{Contributions}

\vspace{.1in}
\noindent {\large \textbf{Intellectual Merit}}
\vspace{.05in}

Our proposed research will contribute to three distinct areas of economics and health policy. First, we contribute to the literature on physician referral networks. This literature typically considers physician networks in the context of shared patients \cite{landon2012, barnett2012mc, landon2018, linde2019}, wherein authors examine social networks of physicians defined as the set of physicians that see the same patients within a designated time period. These studies tend to envision an ``undirected'' physician network, in that the edges (i.e., connections between two physicians) reflect a two-way relationship with patients flowing from one physician to another in both directions. A smaller literature focuses on the ``directed'' graphs in which referring physicians such as PCPs are connected to specialists \cite{agha2017, agha2018, zeltzer2020}.

Our contribution to the literature on physician referral networks is threefold: 1) unlike studies of undirected networks \cite{landon2012, barnett2012mc, landon2018, linde2019}, we envision a directed network in which PCPs exercise significant sway over the flow of services to individual specialists; 2) compared to existing work on directed newtorks \cite{agha2017, agha2018, zeltzer2020}, we consider referrals in the context of orthopedic surgery rather than management of chronic conditions; and 3) while the existing literature in this area takes network structure as given and examines the effects of physician networks on spending and patient health, we will consider how networks are formed and how they evolve over time.

Second, we contribute to the literature on physician learning. The majority of this literature focuses on learning in context of prescription drugs \cite{coscelli2004, crawford2005, ferreyra2011, chan2013, dickstein2018}, wherein authors typically model physician learning based on the physician's own experience, with physicians updating behaviors as they receive more information on the effectiveness and potential side effects of a given drug. Other studies consider learning based on outside sources of information such as the physician's peers or disclosure of physician performance \cite{ho2002,kolstad2013}. In a recent working paper, Gong~\cite{gong2018} studies physician learning in the context of treatment for brain aneurysms, allowing for both skill accumulation (i.e., learning by doing) as well as learning about treatment effectiveness. Authors have also recently examined learning in the context of technology abandonment \cite{berez2018, howard2017}. 

Relatively few papers focus on learning in the context of specialist referrals. In an unpublished working paper, Johnson~\cite{johnson2011} considers PCP learning as a potential mechanism by which specialists receive more or less referrals over time; however, the author does not directly examine the role of learning separate from other potential mechanisms. More recently, Sarsons~\cite{sarsons2018} examines changes in PCP referrals as a function of patient outcomes for different surgeons. She finds that PCPs do substitute away from surgeons with poor patient outcomes, but that the response is larger for female surgeons compared to male surgeons. While both Johnson~\cite{johnson2011} and Sarsons~\cite{sarsons2018} discuss physician referral patterns in response to specialist quality, the authors do not directly assess frictions in learning or produce counterfactual simulations, nor do they examine the evolution of referral networks over several years.

Our third contribution is to the literature on physician agency with regard to referrals. This research extends the traditional role of physician agency to consider the role of physicians not just on the quantity and type of healthcare used, but also on the location of care \cite{baker2016, lin2021nber}. In the context of PCPs, the physician's decision-making authority is most salient in the referral process. Indeed, Freedman et al.~\cite{freedman2015} find that the PCP's recommendation is the most commonly cited reason from a patient in their selection of an oncologist, with similar results documented in Barkowski~\cite{barkowski2018} and Chernew et al.~\cite{chernew2021}. In an unpublished working paper, Walden~\cite{walden2016} examines how hospital acquisitions of primary care practices affect PCP referrals to the hospital. Like Walden~\cite{walden2016}, Barkowski~\cite{barkowski2018}, and Chernew et al.~\cite{chernew2021}, we consider PCP referrals as an important dimension of physician agency; however, our analysis considers physician agency in the context of learning. For example, we will examine whether PCP relationships with other specialists, such as via shared ownership of the practice, act as a barrier to PCP learning.

\vspace{.1in}
\noindent {\large \textbf{Broader Impacts}}
\vspace{.05in}

Beyond our contribution to the economics literature, our results will inform policy-makers as to the potential costs of existing informational frictions in PCP referral networks and the benefits to policies that can successfully reduce these frictions. The main purpose of our structural model is to generate policy simulations that quantify the gains which would be achieved if PCPs could learn about specialist quality more rapidly and thereby improve the allocation of patients to specialists. Here, we present a preliminary descriptive analysis of referrals for orthopedic surgery that suggests significant improvements in health and reductions in costs could be achieved. 

Orthopedic surgery is common, expensive, and involves serious risks. Over 400,000 elderly Medicare beneficiaries undergo a planned and elective major joint replacement each year, accounting for Medicare expenditures of nearly \$5 billion per year. Among these patients, around 0.6\% (about 2,600 patients per year) die within 90 days of their operation, and over 9\% of patients (around 37,500 patients per year) are readmitted within 90 days of discharge. (Based on analysis of our Medicare claims data, described below.)

The risk of a poor health outcome varies dramatically across specialists. Nationwide, among experienced orthopedic surgeons with reasonable volumes, the probability of a failure event---defined as mortality, readmission, or infection---ranges from 1\% to over 20\% per specialist, and the 25th percentile of physician failure rates is less than half of the 75th percentile. Similar variation exists \textit{within} local markets, defined as hospital referral regions (HRRs). This is crucial because patients would be reallocated within markets, not across them. Figure \ref{fig:iqr_quality} shows the differences between the 75th and 25th percentiles of surgeon-specific failure rates by HRR. This figure depicts the hypothetical reduction in failure rates if PCPs could replace referrals to the 75th percentile of specialists (high failure-rate surgeons) with referrals to the 25th percentile of specialists in the failure-rate distribution. The implication of Figure \ref{fig:iqr_quality} is that failure rates could be reduced by between 5 and 10 percentage points in most markets if PCPs were to refer patients to specialists in the lower-quartile of the failure-rate distribution (with a mean rate of 5\%) instead of the higher-quartile (with a mean failure rate of 14\%). We present a similar preliminary analysis of episode spending in Figure \ref{fig:iqr_spending}, which suggests savings of around \$8,000 per 90-day episode when moving from the relatively inefficient (75th percentile of the spending distribution) to relatively efficient (25th percentile of the spending distribution) specialists.

\begin{figure}[h]
\centering
\begin{minipage}{.5\textwidth}
    \centering
    \caption{Potential Quality Improvement \\ (by Hospital Referral Region)}
    \includegraphics[width=\linewidth]{figures/Failure_IQR.png}
  \label{fig:iqr_quality}
\end{minipage}%
\begin{minipage}{.5\textwidth}
    \centering
    \caption{Potential Spending Reduction \\ (by Hospital Referral Region)}
    \includegraphics[width=\linewidth]{figures/Payment_IQR.png}
  \label{fig:iqr_spending}
\end{minipage}
\end{figure}

Focusing on PCP referral networks may offer a relatively practical and actionable way to reduce variation in health care quality and utilization. Viewing physician treatment decisions as the source of variation, which is the common position in much of the literature, necessarily implies that the solution to minimizing health care inefficiencies and improving quality lies in changing physician behaviors. Unfortunately, an established body of research now demonstrates the many barriers to changing physician behaviors \cite{wilensky2016}. Our proposed research envisions an opportunity to improve efficiency and quality not by changing what physicians do, but instead by changing which physicians do it. This could be a more feasible adjustment, and through a deeper understanding of PCP referral networks, it may be possible that such adjustments can be \uline{achieved with minimal burden on patients or physicians and with significant improvements in patient care and spending}.



\vspace{.1in}
\section{Research Plan}

\subsection{Data}
Our proposed analysis relies on four main data sources: 1) the 100\% Medicare claims files (covering all Part A and Part B claims) from 2008 to 2020; 2) information on patient characteristics from the Medicare beneficiary summary files; 3) data on physician practice characteristics from the Medicare Data on Provider Practice and Specialty (MD-PPAS); and 4) data on hospital characteristics from the American Hospital Association (AHA) Annual Surveys. From these data sources, we can identify the following:

\begin{enumerate}
    \item \textbf{Inpatient Surgeries:} Our analysis will focus on PCP referrals to specialists for planned and elective major joint replacements among Medicare beneficiaries aged 65 and above. Planned and elective procedures will be identified from the admission source codes on the inpatient claim. We focus on elective surgeries because they tend to follow a ``standard'' referral process such that we can better identify the referring PCP and associated network of specialists, and we focus on major joint replacements because these are common elective procedures and because joint replacements are  typically performed by a single orthopedic surgeon (rather than a team of specialists that may be involved in other types of care). 
    
    \item \textbf{Referrals:} We will identify PCP referrals in two ways. First, we can directly use the referring physician listed in the claims data, as per Sarsons~\cite{sarsons2018}. Alternatively, we will identify the referring physician based on frequency of ``evaluation and management'' visits over the prior 12-month period before a given surgery. If there is no such unique physician, the PCP with the most recent evaluation and management visit is taken as the referring PCP. This process follows Pham et al.~\cite{pham2009} and Agha et al.~\cite{agha2017} in their assignment of PCPs to patients and has been recently validated in Dugoff et al.~\cite{dugoff2018}.
    
    Hospitals may not list a referring physician for each inpatient stay, and as such, the referring physician field in the claims data is incomplete. Our use of the 100\% claims data will allow us to pursue a combination of these two approaches (identifying a PCP based on frequency of prior visits and based on the referring physician listed in the claims data) in order to fully capture PCP referral networks. An added benefit of this approach is that we can assess the accuracy of identifying referral networks using only the referring physician listed in the claim. The 100\% claims data is important to fully capture PCP referral networks with sample sizes sufficient to quantify learning and PCP responses to specialist quality signals.

    \item \textbf{Quality Outcomes:} In order to form a complete picture of outcomes for each surgery, we will merge the patients identified as part of the surgeries in step 1 to the full population of Parts A and B claims. From there, we will measure quality based on 30/60/90-day readmission, 30/60/90-day mortality, and 30/60/90-day complications. Our measures of complications will include sepsis and surgical site infections, both of which are easily identifiable in the claims data based on ICD-9 and ICD-10 codes.

\end{enumerate}

The unit of observation in our primary dataset will be a patient/procedure. For each patient/procedure, our data will include the operating physician/specialist for the patient's elective surgery, the referring PCP for that surgery, and the quality outcomes for that surgery. We will also capture other measures of spending and health care utilization for each patient from observed claims outside of the initial inpatient stay. 

Our proposed analysis considers referrals from an individual PCP to an individual specialist; however, as part of our sensitivity analysis, we will broaden our view of referrals to that of referrals between practices (defined by tax IDs) rather than individual physicians. Our analysis will also accommodate the potential mediating effect of system affiliation in referral patterns, as prior work from Dr. McCarthy, Dr. Richards-Shubik, and co-authors highlights the role of such affiliation on physician and hospital behaviors \cite{mccarthy2017rio, lin2021, lin2021nber, richards-shubik2021}.

\vspace{.05in}
\subsection{Methods for Examining PCP Referral Networks (Objective 1)}

To examine PCP referral networks, we will focus on the following standard network measures:
\begin{enumerate}
    \item \textit{Network size or degree}: the number of specialists to which a given PCP refers patients.
    \item \textit{Network concentration}: the share of patients that a PCP sends to each specialist in their network, which we then square and sum across specialists for the same PCP in order to form a Herfindahl-Hirschman Index for each PCP. Similar measures of concentration have been used to proxy for care coordination in the management of chronic diseases \cite{agha2018}.
\end{enumerate}

We will then examine the correlation between these measures and other observable physician and market characteristics. This analysis will consist of a series of regressions of the form:
\begin{equation}
    y_{i(m)t} = g\left(x_{it}, z_{mt}, \delta_{t}, \delta_{i}\right) + \varepsilon_{it}
    \label{eqn:aim1_reg}
\end{equation}
where $y_{i(m)t}$ denotes network size or concentration for PCP $i$, in market $m$, at time $t$; $x_{it}$ denotes time-varying physician characteristics, such as their patient mix and system affiliation; $z_{mt}$ denotes market-level characteristics such as the number of hospitals and specialists in the market; $\delta_{t}$ and $\delta_{i}$ denote time and physician-level fixed effects; and $\varepsilon_{it}$ is an error term that is assumed to be additively separable from the conditional mean function, $g()$. We will also consider specifications that exclude $\delta_{i}$ from Equation \eqref{eqn:aim1_reg}, as we suspect that conditioning on the PCP fixed effect may remove some variation in network size across PCPs.

We will estimate Equation \eqref{eqn:aim1_reg} using ordinary least squares with PCP and year fixed effects, based on the generalized within-estimator \cite{correia2017}. For network size, we will also estimate a fixed-effects Poisson regression to better accommodate the count nature of our outcome measure, as well as a parametric model based on a power law distribution. The key independent variables in this analysis are as follows: 
\begin{enumerate}
    \item \textit{Number of specialists}: the count of all specialists in PCP $i$'s market (for a given procedure).
    \item \textit{Availability of specialists}: the \textit{number of specialists} relative to the total number of other patients receiving operations in a given time interval. Other measures of availability include the share or count of other specialists with below-average scheduled operations (based on a look-forward period of 3 or 6 months).
\end{enumerate}

We will also consider an extension to this analysis in which we investigate the \uline{causal effects of specialist markets on PCP referral networks} by exploiting PCPs that newly enter a given market, similar to the analysis of specialist movers in Molitor~\cite{molitor2018}. This is a difference-in-differences (DD) design, in which treated PCPs are those that have moved or recently started practice in a new market, and control PCPs are those that remain in the same market. We will estimate the treatment effect using the recent doubly-robust DD estimator described in Calloway and Sant'anna~\cite{callaway2020}.

Finally, we will examine the relationship between PCP referral networks and health care quality and cost. This analysis considers per patient spending and health outcomes as a function of patient, physician, and hospital characteristics via a series of regressions of the form:
\begin{equation}
    y_{k(ij)t} = g\left(x_{kt}, w_{ht}, z_{ijt}, \delta_{t}, \delta_{i}, \delta_{j} \right) + \varepsilon_{kt}
    \label{eqn:aim1_reg2}
\end{equation}
where $y_{k(ij)t}$ denotes the outcome (spending or quality) for patient $k$, with PCP $i$ and specialist $j$, at time $t$; $x_{kt}$ denotes patient characteristics, such as age, gender, diagnoses, and prior health care utilization; $w_{ht}$ denotes hospital characteristics based on the patient's admitting hospital; $z_{ijt}$ denotes PCP-specialist measures such as network size and network concentration, as well as characteristics of each PCP and specialist separately; $\delta_{t}$, $\delta_{i}$, and $\delta_{j}$ denote time, PCP, and specialist fixed effects, respectively; and $\varepsilon_{kt}$ is an error term that is assumed to be additively separable from the conditional mean function, $g()$. We will also consider specifications that exclude $\delta_{i}$ and $\delta_{j}$ in order to capture the relationship between referrals and specialist practice styles. For example, since we anticipate practice styles to be relatively stable over time, specifications of Equation \eqref{eqn:aim1_reg2} that include $\delta_{j}$ cannot inform as to the relationship between referrals and specialist practice style.

For our spending measures, we will estimate Equation \eqref{eqn:aim1_reg2} using the generalized within-estimator \cite{correia2017}. For our quality measures, we will estimate logistic regression models to accommodate the binary outcome of each surgery. The results will quantify the relationship between PCP/specialist network measures and patient-level spending and health outcomes. For example, it may be that that small PCP referral networks or heavily concentrated networks are associated with worse health outcomes, since a small and concentrated network may reflect an unwillingness to experiment with different specialists in the PCP's market. Alternatively, small and concentrated referral networks may reflect an equilibrium outcome of PCP learning about specialist quality, in which PCPs have sequentially ruled-out lower quality specialists over time. In this case, smaller and more concentrated networks may be associated with better health outcomes. Investigating heterogeneities by PCP experience, and focusing specifically on PCP movers, will help to separately identify these two possible mechanisms within the reduced-form setting of Objective 1.

\vspace{.05in}
\subsection{Methods for Examining PCP Learning (Objective 2)}

We will use the framework of a multi-armed bandit to model how referring physicians choose specialists and learn about their quality over time. This framework, which has been applied previously to study physician decisions about alternative medications \cite{dickstein2018} and procedures \cite{gong2018}, specifies a set of options whose payoffs are not precisely known. The physician repeatedly chooses among the options over time, in our case by referring patients to various specialists, and learns about the distribution of payoffs from each option based on the outcomes that occur.

There are two main benefits from developing and estimating the structural model. First, the model provides clear guidance for an empirical specification, and it helps to elucidate econometric concerns about the specification. Second, once estimated, the structural model will enable counterfactual simulations of the potential benefits from improved learning.

\vspace{.05in}
\subsection*{Model Elements}

We consider a PCP, $i$, who refers patients to specialists from a set of available specialists in the market, $j \in J$. In the theoretical model, the PCP has one patient per time period, so patients and time are both denoted with $t$. The choice of specialist is denoted with a set of indicators, $D_{ijt}, j \in J$, where $D_{ijt} = 1$ if patient $t$ is sent to specialist $j$, otherwise $D_{ijt} = 0$. The health outcome for the patient is binary, denoted $Y_{ijt}$, with $Y_{ijt}=1$ for success and  $Y_{ijt}=0$ for failure (i.e., complication, readmission, or death). 

Our model will capture three key factors in the referral decision:
\begin{enumerate}
    \item \textit{Learning:} The probability of success differs across specialists, and is denoted $p_j$. The PCP does not know $p_j$ but has beliefs that are updated by experience based on the outcomes of their patients.
    \item \textit{Relationships:} PCPs may prefer to work with specialists with whom they have shared more patients. The number of patients sent to specialist $j$, before the current patient $t$, is denoted $e_{ijt}$.
    \item \textit{Capacity Constraints:} Specialists cannot treat an unlimited number of patients within each finite time period, so the PCP may be unable to send the patient to the most preferred specialist. The total number of patients seen by specialist $j$ in period $t$, which depends on exogenous referrals from other PCPs, is denoted $n_{jt}$
\end{enumerate}

These features will enable us to differentiate among important possible sources of inefficiency in referrals. One is that the learning process may be too myopic, which implies that PCPs do not experiment enough among the available specialists before settling on their preferred set of providers. Another is that PCPs may enjoy working with familiar specialists and consequently may exhibit inertia for remaining in established relationships. These mechanisms would have different empirical implications because they depend differently on the number of successes and failures vs.~the total number of patients. Last, allowing for capacity constraints may be important because they can limit the extent to which referrals are able respond to provider quality \cite{richards-shubik2021}.

\vspace{.05in}
\subsection*{Learning Process}

The PCP uses Bayesian inference to learn about the specialists' success probabilities. The beliefs about $p_j$ are specified as a beta distribution with parameters $(a, b)$, which is a natural and tractable modeling choice when outcomes are binary. In the initial period, before any patients have been sent to any specialists, these parameters are equal to $(a_0, b_0)$.  This defines a common prior about the quality of the specialists in the market, and it determines the ``stickiness'' of beliefs (larger values of the initial parameters make beliefs less responsive to outcomes). The parameters are updated based on the numbers of successes and failures among the patients sent to specialist $j$, as follows:
\begin{equation*}
a_{ijt} = a_0 + \sum_{s=1}^t Y_{ijs} \ \ \text{ and } \ \ b_{ijt} = b_0 + \sum_{s=1}^t (D_{ijs} - Y_{ijs}).
\end{equation*}
The mean and variance of the beta distribution are simple expressions of these parameters; specifically, the mean and variance of the beliefs about $p_j$ in period $t$, which use the history up to period $t-1$, are as follows:
\begin{equation}
m_{ijt} \equiv \frac{ a_{ij,t-1} }{ a_{ij,t-1} + b_{ij,t-1} } \text{ and }
v_{ijt} \equiv \frac{ a_{ij,t-1} b_{ij,t-1} }{ (a_{ij,t-1} + b_{ij,t-1})^2 (a_{ij,t-1} + b_{ij,t-1} + 1) } . \label{eqn:mean_var}
\end{equation}
Thus, for example, the Bayesian expectation of the probability of success for patient $t$ at specialist $j$ is equal to $m_{ijt}$, which is a function of the initial parameters $(a_0, b_0)$ and the outcomes among the patients previously sent to that specialist.
More successes make $a_{ij,t-1}$ larger, hence make $m_{ijt}$ higher.  
However, larger values of $a_0$ and $b_0$ make $m_{ijt}$ less responsive to patient outcomes (i.e., ``stickier'' beliefs).  If on the other hand $a_0$ and $b_0$ are close to zero, then $m_{ijt}$ is essentially equal to the success rate among the patients sent to specialist $j$ in the past. 

PCPs value the outcomes for their patients, along with the relationships they have with specialists. The outcome is not known when the referral is made, so the current expectation $m_{ijt}$ is used.  This is weighted by a parameter $\alpha$, which represents the PCP's altruism or concern for patient health. The value of familiarity is given by a function of the number of previous patients sent to the specialist, $f(e_{ijt})$, where $f$ is increasing and concave, and the number of previous patients is $e_{ijt} = \sum_{s=1}^{t-1} D_{ijs}$.
The PCP also values certain observable factors, $x_{ijt}$, that may affect the patient's utility from specialist $j$, for example distance.  These enter the PCP's utility as $u(x_{ijt})$. There are also factors not observable to the econometrician.
Our data and approach make it possible to include a fixed effect for each specialist, $\xi_j$, which represents quality and demand factors that affect all patients and PCPs.
In addition there are idiosyncratic unobservable factors, $\epsilon_{ijt}$, which have a known parametric distribution (e.g., type I extreme value). 
Finally, in the specification that accounts for capacity constraints, the effect of congestion is approximated with an effect of the patient volume, $c(n_{jt})$. Bringing these together, the utility that PCP $i$ perceives from sending patient $t$ to specialist $j$ is
\begin{equation}
U_{ijt} \equiv \alpha m_{ijt} + f(e_{ijt}) + u(x_{ijt}) + c(n_{jt}) + \xi_j + \epsilon_{ijt}.
\label{eqn:learning_utility}
\end{equation}

\vspace{0.05in}
\subsection*{Myopic and Forward-Looking Behavior}

A myopic PCP simply chooses the specialist with the highest expected payoff for the current patient:
\begin{equation} \label{eqn:myopic}
\max_j \ \text{E} \left[ U_{ijt} | \dots \right]
= \max_j \left\{ \alpha m_{ijt} + f(e_{ijt}) + u(x_{ijt}) + c(n_{jt}) + \xi_j + \epsilon_{ijt} \right\} .
\end{equation}
Thus if PCPs are myopic, they unambiguously tend to refer to specialists with whom they have had more successes in the past. 

If PCPs are forward-looking, they also value experimenting with relatively unknown specialists. The choice of specialist in period $t$ involves both the utility for the current patient and the value of learning more about the quality of specialists in the market, which could benefit future patients.  The solution to this dynamic problem simplifies with the use of a \emph{Gittins index} \cite{gittins1979}, which expresses the value of learning about specialist $j$ as a function of the mean and variance of the current beliefs about that specialist. We denote this abstractly as $g(m_{ijt}, v_{ijt})$, but the function is well approximated with a fairly simple and tractable closed-form expression \cite{brezzi2002}. This function replaces the expectation of success for the current patient.  
Accordingly, with forward-looking behavior, the maximization problem is
\begin{equation} \label{eqn:dynamic}
\max_j \ \text{E} \left[ V_{ijt} | \dots \right]
= \max_j \left\{ \alpha g(m_{ijt}, v_{ijt}) + \overline{\overline{f}}(e_{ijt}) + u(x_{ijt}) + c(n_{jt}) + \xi_j + \epsilon_{ijt} \right\} .
\end{equation}
The key difference with myopic behavior is the term $g(m_{ijt}, v_{ijt})$, which is increasing in the variance. The forward-looking model therefore assigns some value to trying specialists with whom the PCP may have less prior experience, because the variance is decreasing in the number of prior patients (i.e., $v_{ijt}$ in \eqref{eqn:mean_var} is decreasing in $a_{ij,t-1} + b_{ij,t-1}$). In addition, the PCP considers the present discounted value of familiarity with the specialist for future patients, not just the current value, which is indicated by replacing $f(e_{ijt})$ with $\overline{\overline{f}}(e_{ijt})$ above.

\begin{table}[ht]
\centering
\footnotesize
\begin{minipage}[h]{6in}
\caption[caption]{\textbf{Descriptive Statistics for PCPs and PCP/Specialist Pairs}\footnote{Mean values calculated per year, with standard deviations in parenthesis. The period from 2008-2012 is our baseline period used to form histories of PCP/specialist pairs over a common time period, and the period from 2013-2018 is our estimation period. We present summary statistics separately by those time periods for consistency with our preliminary estimation.}}
\centerline{%
    \begin{tabular}{lrrr}
        \input{tables/sum-stats-pairs.tex}
    \end{tabular}
}
\label{tab:sum-pairs}
\end{minipage}
\end{table}

\vspace{0.05in}
\subsection*{Identification and Estimation}

The next major step in the development of the model is to complete the empirical specification. The identifiability of key features is potentially subtle.  For example, the mean and variance of the beliefs ($m_{ijt}$ and $v_{ijt}$) and the experience with a specialist ($e_{ijt}$) are all functions of the number of patients sent to the specialist and the number of successes. Hence it will be important to choose a specification for $f(e_{ijt})$ that allows sufficient variation in that term, independent of the Gittins index, $g(m_{ijt}, v_{ijt})$.  Once the specification is completed, estimation can be accomplished via common nonlinear optimization methods.

\vspace{.2in}
\subsection{Preliminary Results}

We currently have access to Medicare claims data from 2008-2018. In order to form a common baseline time period, we use the five year period from 2008-2012 to construct a running count of patients and failure events for each PCP/specialist pair. We then take the six year period from 2013-2018 as our estimation period. 


Based on the physician's NPI, we identify around 21,200 unique PCPs and 11,600 unique specialists over our estimation period (2013-2018). As summarized in Table \ref{tab:sum-pairs}, each PCP refers an average of 3.5 patients for elective orthopedic surgery per year with an average network size (or, \emph{degree}) of just under 2.5 orthopedic specialists per year. PCPs therefore refer an average of roughly 1.4 patients to a given specialist in a year, for specialists to whom they send any patients in that year. Table \ref{tab:sum-pairs} also presents average total failures associated with a PCP's patients and the failure rates per referral. Failures are not common, but there are 0.33 failures among a PCP's patients each year on average, so PCPs would see these bad outcomes occasionally.  Per referral the failure rate is about 0.09; in other words, around 9\% of a PCP's referrals for major joint replacements result in some form of failure, defined as a death, readmission, or complication within 90 days of discharge.


\vspace{.1in}
\subsection*{PCP Referral Networks}

\begin{wrapfigure}[18]{R}{0.65\textwidth}
\centering
\begin{minipage}[h]{4in}
\caption[caption]{PCP Referral Network Size\footnote{Figure depicts the frequency histogram of PCP network size over the full 2013-2018 estimation period.}}
\centerline{%
    \includegraphics[scale=0.5]{figures/NetworkSize.png}
}
\label{fig:network-size}
\end{minipage}
\end{wrapfigure}


The distribution of PCP referral network sizes combining the six years from 2013 to 2018 is presented in Figure \ref{fig:network-size}. This shows the total number of unique specialists to which a PCP refers patients over the estimation period, after dropping PCPs with the highest 1\% network sizes (larger than 50). Consistent with the yearly average network sizes in Table \ref{tab:sum-pairs}, the typical PCP sends patients to relatively few specialists (around 7) over the entire estimation period. The frequency distribution also reveals a long tail, illustrating the presence of a small set of PCPs with very large network sizes.


Conditional on a small network size, referrals among a given PCP's network are relatively equally distributed, with a PCP's most commonly referred specialist accounting for around 35\% of all referrals from that PCP over the estimation period. Figure \ref{fig:network-share} presents the distribution of these ``highest-share'' specialists across PCPs, where we calculate each specialist's share of a given PCP's total referrals from 2013 to 2018 and take the maximum of these shares for each PCP. As is evident from Figure \ref{fig:network-share}, most PCP referral networks are evenly distributed among a small set of specialists; however, there is a non-negligible set of PCPs that isolate their referrals to just a single specialist.


\vspace{.1in}
\subsection*{Physician Learning}

Recall that a myopic PCP simply chooses the specialist with the highest expected payoff for the current patient, as reflected in Equation \eqref{eqn:myopic}. In this myopic case, physicians unambiguously tend to refer to specialists with whom they have had more successes in the past. The reduced form of Equation \eqref{eqn:myopic} is a discrete-choice model with specialist fixed effects. The main difference with the structural learning model is that the reduced form does not include the parameters for the initial prior beliefs, $(a_0, b_0)$. Instead, the coefficient on the outcome variable, $m_{ijt}$, reflects both the concern for patient health (the parameter $\alpha$) and the prior beliefs.

\begin{wrapfigure}[19]{R}{0.65\textwidth}
\centering
\begin{minipage}[t]{4in}
\caption[caption]{Highest Referral Shares over 2013-2018\footnote{Figure depicts the frequency histogram of the ``highest-share'' specialist per PCP over the full 2013-2018 estimation period.}}
\centerline{%
    \includegraphics[scale=0.5]{figures/HighestShare.png}
}
\label{fig:network-share}
\end{minipage}
\end{wrapfigure}


Taking the reduced form to the data, we use the failure rate among past patients as the outcome variable, $m_{ijt}$, and we use the proportion of past patients sent to the specialist as the familiarity variable, $e_{ijt}$. We also include in our specification the interaction between these variables, $m_{ijt} \times e_{ijt}$. The distance between the patient and the hospital where the specialist primarily operates is $x_{ijt}$, and we include an indicator for whether the PCP and specialist are in the same group practice, denoted $z_{ijt}$, based on tax identification numbers on their claims. This could capture another form of familiarity, or various cost reductions and other incentives. 

Each choice set, $J_{i}$, consists of all specialists observed to have operated on patients from the same HRR in the same year. We then exclude any specialists that operate in hospitals 150 miles or further away from the patient. The resulting mean choice set contains about 32 unique specialists. For each specialist in the choice set, we construct a running variable of the number of prior patients referred to that specialist $j$ by PCP $i$, and the number of failures among those patients. We then divide by the total number of prior patients referred by PCP $i$ to any specialist, to obtain the proportion of past patients sent to $j$ ($e_{ijt}$) and the failure rate among those patients ($m_{ijt}$). These running variables reflect the pairwise history of each PCP/specialist, and we limit this history to a five year period. 

For example, consider a referral from PCP $i$ to specialist $j$ for an inpatient admission on January 1, 2017. Our running variables for this referral are therefore constructed based on the observed referrals and failures among the same PCP/specialist pair over the period from January 1, 2012 through December 31, 2016. Referrals or failures occurring more than five years prior are assumed not to affect the probability of the current referral. While the five-year time frame is relatively subjective, some cutoff is necessary in order to ensure a common history for all PCP/specialist pairs. We selected five years as it splits our available data into two periods: 2008-2012 is our baseline period over which we can construct a common history for each PCP/specialist pair, and 2013-2018 is our estimation period. 

The specialist fixed effects, $\xi_j$, are estimated, which is possible because there is a large number of patients in each HRR but a fixed and relatively small number of specialists within each HRR. The errors $\epsilon_{ijt}$ are assumed to have a type I extreme value distribution, and all variables enter linearly. This yields a typical multinomial logit specification, as follows:
\begin{equation} 
\label{eqn:reduced}
\Pr \left( D_{ijt}=1 \right | \dots ) = \frac{\exp \left( \pi' (m_{ijt}, e_{ijt}, x_{ijt}, z_{ijt}) + \xi_j\right)}{\sum_{k \in J_{i}} \exp \left( \pi' (m_{ikt}, e_{ikt}, x_{ikt}, z_{ikt}) + \xi_k \right)},
\end{equation}
where $J_{i}$ denotes the choice set of specialists available to PCP $i$. We estimate the vector of coefficients $\pi$ and the fixed effects $\xi$ via maximum likelihood. We do so separately by HRR, and we aggregate our estimates by taking the average across HRRs weighted by patient counts.

The use of specialist fixed effects provides fairly robust identification of learning effects. With the fixed effects, the identifying variation for the estimated response to patient outcomes comes from differences \emph{across PCPs} in the failure rates among the patients they have sent the same specialist. Unobserved factors that drive a specialist's overall volume, which would contaminate the estimated response if they are correlated with patient outcomes, are absorbed by the fixed effects.  In addition, our long panel of data enables us to use the timing of events by considering the effect of past outcomes on future referrals, which further supports the interpretation the estimated response as a learning effect.

\vspace{0.05in}
\paragraph{Results across HRRs:} 

\begin{figure}[h]
\centering
\begin{minipage}[h]{5in}
\caption[caption]{Marginal Effects of Failure Rate on Referral Probability\footnote{Figure shows distribution across HRRs of average marginal effects of failure rate on referral probability to highest-share specialist. Marginal effects computed from multinomial logit models estimated separately for each HRR and including specialist fixed effects.}}
\centerline{%
    \includegraphics[scale=0.5]{figures/mfx_failures_hist.png}
}
\label{fig:mfx}
\end{minipage}
\end{figure}

Figure \ref{fig:mfx} plots the distributions of the marginal effects of the failure rate across HRRs when including specialist fixed effects. Specifically these are the average marginal effects on a PCP's highest-share specialist (i.e., the one most frequently chosen), which incorporate the main effect and the interaction term, computed separately for each of the 276 HRRs in our analysis. From this preliminary analysis, 33/276 (12\%) of the HRRs yield a statistically significant negative marginal effect. However when combining the estimates from all the HRRs, by taking a weighted average using the number of patients in each HRR, the national average marginal effect is strongly significant, equal to -0.0644 with a standard error of 0.0057. As summarized in Table \ref{tab:national-mfx}, the marginal effects are roughly twice as large without specialist fixed effects, and their national average is -0.1202 with a standard error of 0.0061. Because the latter estimates do not include the fixed effects, $\xi_j$, they may reflect other factors that generate a positive association between market shares and patient outcomes.  For example, if specialists at hospitals with better amenities (higher $\xi_j$) tend to have better outcomes (lower $m_{ijt}$), then these marginal effects would be inflated because of the unmeasured effects of amenities on referrals. The specialist fixed effects address such sources of bias.

To interpret the magnitude of the marginal effects, first note there is an average of 3.35 referrals per PCP-specialist pair over 5 years, so having one additional bad outcome (e.g., a readmission) among the patients sent by a PCP to a particular specialist would increase the failure rate for that pair by about 0.3. The marginal effect of -0.06 implies that this would reduce the referral probability by $0.3 \times 0.06 = 0.018$. Relative to the average probability of a referral to the highest-share specialist, which is about 0.4, is this a 4.5\% relative reduction.  While this effect is somewhat small, it is not trivial, and is in line with other estimates in the literature on the response to provider quality.

\begin{table}[t]
\centering
\footnotesize
\begin{minipage}[h]{6in}
\caption[caption]{\textbf{National Average Marginal Effects on Referral Probabilities}\footnote{Means (standard errors in parenthesis) of the estimated marginal effects from each HRR.}}
\centerline{%
    \begin{tabular}{lcc}
        \input{tables/national-marginal-effects.tex}
    \end{tabular}
}
\label{tab:national-mfx}
\end{minipage}
\end{table}


\vspace{.2in}
\subsection{Limitations}
The central limitation to our empirical analysis is the potential role of unobserved factors that shape a PCP's established relationships and that also affect the probability of referral for any given patient. For example, based on prior relationships, a PCP may be aware of certain patients for whom a given specialist is a better or worse match. If a PCP makes this determination based on factors unobserved to the econometrician, then it may bias our estimates away from identifying any effect of learning or any response of PCPs to specialist quality signals. 

We will attempt to overcome this limitation with a series of alternative identification strategies and specifications. For example, we will consider a subset of PCPs that have newly entered a given market, in which case existing relationships and any unobservable factors therein are less problematic. Similarly, we will exploit plausibly exogenous ``shocks'' to PCP referral networks as a source of variation in PCP referrals. Examples of such shocks include new specialists that enter a market or existing specialists leaving the market. 

A related empirical issue concerns the appropriate ``level'' at which referrals are made and how information about quality outcomes is observed. For example, PCPs may refer to a specialist practice rather than an individual specialist. Relatedly, the quality outcomes of a given specialist may flow through the PCPs practice rather than to an individual PCP, or such quality information may not be sufficiently salient so that it is not observed by the PCP at all. Our analysis will attempt to address these limitations by: considering referrals at the practice level, rather than physician-to-physician; considering other clinical conditions where severe negative outcomes are more likely, such as elective coronary artery bypass graft (CABG); and identifying referrals for which we observe a follow-up visit to the same PCP, in which case we can be more confident that the PCP would be aware of any readmissions or complications.

Another potential limitation is the use of Medicare fee-for-service claims data, which may capture a different referral network than would be observed in other insurance markets (albeit a referral network free from any network restraints placed by the insurer). Addressing this limitation would require a significant investment in additional data. As such, we will not directly address this issue in the current research plan; however, our long-term goals include examining heterogeneities in referral patterns across different insurance markets.

\vspace{.1in}
\section{Results from Prior NSF Support}
\vspace{.05in}

None to date.

\clearpage
\newpage

%\bibliographystyle{authordate1}
\bibliographystyle{unsrt}
\bibliography{BibTeX_Library}

\end{document} 
